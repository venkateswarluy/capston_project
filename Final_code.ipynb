{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up environement\n",
    "import os\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/share/spark-3.1.1-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing All required packages\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,ArrayType,DateType,FloatType,TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining sparksession\n",
    "spark = SparkSession.builder.appName('SparkSQL_UseCase').master('local[*]').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specifying File Schemas (i.e. Mentioning column name, datatype and Null value status of each column):-\n",
    "\n",
    "#1 Aisles Schema:-\n",
    "aisles_schema= StructType([StructField('aisle_id',IntegerType(),False),\n",
    "                          StructField('aisle',StringType(),True)])\n",
    "\n",
    "#2 Departments_schema:-\n",
    "department_schema=StructType([StructField('department_id',IntegerType(),False),\n",
    "                                StructField('department',StringType(),True)])\n",
    "#3 order_schema:-\n",
    "orders_schema=StructType([StructField('order_id',IntegerType(),False),\n",
    "                              StructField('user_id',IntegerType(),True),\n",
    "                              StructField('eval_set',StringType(),True),\n",
    "                              StructField('order_number',IntegerType(),True),\n",
    "                              StructField('order_dow',IntegerType(),True),\n",
    "                              StructField('order_hour_of_day',IntegerType(),True),\n",
    "                              StructField('days_since_prior_order',IntegerType(),True)])\n",
    "\n",
    "#4 prior_order_schema and train_order_schema:-\n",
    "prior_order_schema=StructType([StructField('order_id',IntegerType(),True),\n",
    "                              StructField('product_id',IntegerType(),True),\n",
    "                              StructField('add_to_cart_order',IntegerType(),True),\n",
    "                              StructField('reordered',IntegerType(),True)])\n",
    "#5 Products_schema:-\n",
    "products_schema=StructType([StructField('product_id',IntegerType(),False),\n",
    "                              StructField('product_name',StringType(),True),\n",
    "                              StructField('aisle_id',StringType(),True),\n",
    "                              StructField('department_id',StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Extracting Data:-\n",
    "#defining file path from where to read the files and output path\n",
    "#Note:- The data has been copied to local and then given the local path here as I was facing issues with Insofe cluster for my IP.\n",
    "dataset_path='/home/fai10101/Project/Data_sets/'\n",
    "output_path=dataset_path+\"/output/\"\n",
    "\n",
    "#reading files as dataframes:-\n",
    "\n",
    "#aisles\n",
    "aisles_df = spark.read\\\n",
    "        .schema(aisles_schema)\\\n",
    "        .option(\"delimeter\",\",\").option(\"header\",\"True\")\\\n",
    "        .csv(dataset_path+'aisles.csv')\n",
    "\n",
    "#departments:-\n",
    "department_df = spark.read\\\n",
    "                .schema(department_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'departments.csv')\n",
    "#orders:-\n",
    "orders_df = spark.read\\\n",
    "                .schema(orders_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'orders.csv')\n",
    "\n",
    "#prior_order:-\n",
    "prior_order_df = spark.read\\\n",
    "                .schema(prior_order_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'prior_order.csv')\n",
    "\n",
    "#products:- reading products file as rdd as it has some noises later on it has been converted to data frame after removing noises. \n",
    "#All other files have been read as csv\n",
    "products_rdd = spark.sparkContext\\\n",
    "                .textFile(dataset_path+'products.csv')\n",
    "\n",
    "#train_order:-\n",
    "train_order_df= spark.read\\\n",
    "                .schema(prior_order_schema)\\\n",
    "                .option(\"header\",\"True\")\\\n",
    "                .csv(dataset_path+'train_order.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing noises from products data:- removing unwanted characters from records like:- '\\' , '\"' , ',' etc\n",
    "#after removing noises we will convert it to dataframe\n",
    "def remove_noise(row):\n",
    "    if '\"' in row:\n",
    "        first=row.index('\"')\n",
    "        last=row.index('\"',first+1)\n",
    "        part_a=row[0:first]\n",
    "        part_b=row[first:last+1].replace(\", \",\" - \").replace('\"','')\n",
    "        part_c=row[last+1:]\n",
    "        row=(part_a+part_b+part_c).replace('\\\"',\"\").split(\",\")\n",
    "        return [int(row[0]),row[1],row[2],row[3]]\n",
    "    else:\n",
    "        row = row.replace('\\\"',\"\").split(\",\")\n",
    "        return [int(row[0]),row[1],row[2],row[3]]\n",
    "\n",
    "header=products_rdd.first()\n",
    "products_rdd_mo=products_rdd.filter(lambda x : x!=header).map(lambda x : remove_noise(x))\n",
    "products_df=products_rdd_mo.toDF(products_schema) # product dataframe creation from product rdd after removing noises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing columns of all data frames:\n",
      "\n",
      "aisles :\n",
      " ['aisle_id', 'aisle']\n",
      "\n",
      "products :\n",
      " ['product_id', 'product_name', 'aisle_id', 'department_id']\n",
      "\n",
      "departments :\n",
      " ['department_id', 'department']\n",
      "\n",
      "orders :\n",
      " ['order_id', 'user_id', 'eval_set', 'order_number', 'order_dow', 'order_hour_of_day', 'days_since_prior_order']\n",
      "\n",
      "prior order :\n",
      " ['order_id', 'product_id', 'add_to_cart_order', 'reordered']\n",
      "\n",
      "train order :\n",
      " ['order_id', 'product_id', 'add_to_cart_order', 'reordered']\n"
     ]
    }
   ],
   "source": [
    "#showing columns of all data frames:\n",
    "print('Showing columns of all data frames:')\n",
    "print('\\naisles :\\n',aisles_df.columns)\n",
    "print('\\nproducts :\\n',products_df.columns)\n",
    "print('\\ndepartments :\\n',department_df.columns)\n",
    "print('\\norders :\\n',orders_df.columns)\n",
    "print('\\nprior order :\\n',prior_order_df.columns)\n",
    "print('\\ntrain order :\\n',train_order_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Showing Data types of all data frames:\n",
      "Aisles\n",
      "root\n",
      " |-- aisle_id: integer (nullable = true)\n",
      " |-- aisle: string (nullable = true)\n",
      "\n",
      "Department\n",
      "root\n",
      " |-- department_id: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n",
      "Products\n",
      "root\n",
      " |-- product_id: integer (nullable = false)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- aisle_id: string (nullable = true)\n",
      " |-- department_id: string (nullable = true)\n",
      "\n",
      "Orders\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- eval_set: string (nullable = true)\n",
      " |-- order_number: integer (nullable = true)\n",
      " |-- order_dow: integer (nullable = true)\n",
      " |-- order_hour_of_day: integer (nullable = true)\n",
      " |-- days_since_prior_order: integer (nullable = true)\n",
      "\n",
      "Prior Order\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- add_to_cart_order: integer (nullable = true)\n",
      " |-- reordered: integer (nullable = true)\n",
      "\n",
      "Train Order\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- add_to_cart_order: integer (nullable = true)\n",
      " |-- reordered: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#showing Data types of all data frames:\n",
    "\n",
    "print('\\nShowing Data types of all data frames:')\n",
    "print('Aisles')\n",
    "aisles_df.printSchema()\n",
    "print('Department')\n",
    "department_df.printSchema()\n",
    "print('Products')\n",
    "products_df.printSchema()\n",
    "print('Orders')\n",
    "orders_df.printSchema()\n",
    "print('Prior Order')\n",
    "prior_order_df.printSchema()\n",
    "print('Train Order')\n",
    "train_order_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B) Transformation:- Data Processing Part\n",
    "\n",
    "#Creating Tables from dataframes for aggregation purposes:-\n",
    "aisles_df.createOrReplaceTempView('aisles') # aisles table\n",
    "department_df.createOrReplaceTempView('department') # department table\n",
    "orders_df.createOrReplaceTempView('orders') # orders table\n",
    "prior_order_df.createOrReplaceTempView('prior_order') #prior_order table\n",
    "products_df.createOrReplaceTempView('products') #products table\n",
    "train_order_df.createOrReplaceTempView('train_order') #train_order table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying records from tables\n",
      "+--------+--------------------+\n",
      "|aisle_id|               aisle|\n",
      "+--------+--------------------+\n",
      "|       1|prepared soups sa...|\n",
      "|       2|   specialty cheeses|\n",
      "+--------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------+----------+\n",
      "|department_id|department|\n",
      "+-------------+----------+\n",
      "|            1|    frozen|\n",
      "|            2|     other|\n",
      "+-------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "| 1363380|     50|   prior|           1|        3|                9|                  null|\n",
      "| 3131103|     50|   prior|           2|        6|               12|                  null|\n",
      "+--------+-------+--------+------------+---------+-----------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------+----------+-----------------+---------+\n",
      "|order_id|product_id|add_to_cart_order|reordered|\n",
      "+--------+----------+-----------------+---------+\n",
      "|      12|     30597|                1|        1|\n",
      "|      12|     15221|                2|        1|\n",
      "+--------+----------+-----------------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+--------------------+--------+-------------+\n",
      "|product_id|        product_name|aisle_id|department_id|\n",
      "+----------+--------------------+--------+-------------+\n",
      "|         1|Chocolate Sandwic...|      61|           19|\n",
      "|         2|    All-Seasons Salt|     104|           13|\n",
      "+----------+--------------------+--------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------+----------+-----------------+---------+\n",
      "|order_id|product_id|add_to_cart_order|reordered|\n",
      "+--------+----------+-----------------+---------+\n",
      "|    1077|     13176|                1|        1|\n",
      "|    1077|     39922|                2|        1|\n",
      "+--------+----------+-----------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying the records from tables\n",
    "print('\\nDisplaying records from tables')\n",
    "spark.sql(\"select * from aisles\").show(2)\n",
    "spark.sql(\"select * from department\").show(2)\n",
    "spark.sql(\"select * from orders\").show(2)\n",
    "spark.sql(\"select * from prior_order\").show(2)\n",
    "spark.sql(\"select * from products\").show(2)\n",
    "spark.sql(\"select * from train_order\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#aggregating products, prior_order and train_order data first (just to make the process easy abd simple)\n",
    "aggregated_table_part_1 =spark.sql('''SELECT p.product_id, product_name, aisle_id, department_id, order_id, add_to_cart_order, reordered\n",
    "                                      FROM products p INNER JOIN train_order to ON to.product_id=p.product_id\n",
    "                                      UNION ALL\n",
    "                                      SELECT p.product_id, product_name, aisle_id, department_id, order_id,add_to_cart_order,reordered\n",
    "                                      FROM products p INNER JOIN prior_order po ON po.product_id=p.product_id''')\n",
    "\n",
    "#creating table from aggregated_table_part_1 dataframe for further aggregation\n",
    "aggregated_table_part_1.createOrReplaceTempView(\"Combined_table\")\n",
    "\n",
    "#aggregating all tables as per the data model\n",
    "fully_combined_table = spark.sql('''SELECT product_id, product_name, t.aisle_id,aisle, d.department_id, department, o.order_id, user_id, \n",
    "                                    add_to_cart_order, reordered,eval_set, order_number, order_dow, order_hour_of_day, days_since_prior_order\n",
    "                                   FROM Combined_table t \n",
    "                                   INNER JOIN orders o ON o.order_id=t.order_id \n",
    "                                   INNER JOIN aisles a ON a.aisle_id=t.aisle_id\n",
    "                                   INNER JOIN department d ON d.department_id=t.department_id''')\n",
    "\n",
    "                \n",
    "#C Loading results to destination:- writing back tranformed data to destination (data lake):-\n",
    "\n",
    "fully_combined_table.coalesce(1).write.option(\"header\",True).csv(output_path)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Value count:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aisle_id': 0, 'aisle': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking Null Values\n",
    "print('Null Value count:')\n",
    "{col : aisles_df.filter(aisles_df[col].isNull()).count() for col in aisles_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'department_id': 0, 'department': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{col : department_df.filter(department_df[col].isNull()).count() for col in department_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product_id': 0, 'product_name': 0, 'aisle_id': 0, 'department_id': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{col : products_df.filter(products_df[col].isNull()).count() for col in products_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order_id': 0,\n",
       " 'user_id': 0,\n",
       " 'eval_set': 0,\n",
       " 'order_number': 0,\n",
       " 'order_dow': 0,\n",
       " 'order_hour_of_day': 0,\n",
       " 'days_since_prior_order': 82683}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{col : orders_df.filter(orders_df[col].isNull()).count() for col in orders_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order_id': 0, 'product_id': 0, 'add_to_cart_order': 0, 'reordered': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{col : prior_order_df.filter(prior_order_df[col].isNull()).count() for col in prior_order_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'order_id': 0, 'product_id': 0, 'add_to_cart_order': 0, 'reordered': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{col : train_order_df.filter(train_order_df[col].isNull()).count() for col in train_order_df.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
